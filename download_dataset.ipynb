{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import shutil\n",
    "import gzip\n",
    "import bz2\n",
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code for text dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://dumps.wikimedia.org/enwiki/'\n",
    "res = requests.get(url)\n",
    "content = str(res.text)\n",
    "html_soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "#print(html_soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['202003', '202002', '202001', '201912']\n"
     ]
    }
   ],
   "source": [
    "containers = html_soup.find_all('pre')\n",
    "links = containers[0].findAll('a', href=True, text=True)\n",
    "link_list = []\n",
    "for link in links:\n",
    "    link_list.append(link['href'].replace('/', '')[:6])\n",
    "    \n",
    "# currently available text datasets\n",
    "available_month = sorted(set(link_list[1:-1]), reverse=True)\n",
    "print(available_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example input1\n",
    "input_date = '202001'\n",
    "\n",
    "# default: the first dataset of the month as we'll update the dataset in monthly basis\n",
    "file_page = url+input_date+'01/'\n",
    "\n",
    "res_page = requests.get(file_page)\n",
    "content_page = str(res_page.text)\n",
    "html_page = BeautifulSoup(content_page, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n"
     ]
    }
   ],
   "source": [
    "container_link = html_page.find_all('li', class_='file')\n",
    "file_list = []\n",
    "for link in container_link:\n",
    "    if ('multistream' in link.text.split(' ')[0]) and ('.xml' in link.text.split(' ')[0]):\n",
    "        file_list.append(link.text.split(' ')[0])\n",
    "\n",
    "available_files = file_list[1:]\n",
    "print(len(available_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2\n"
     ]
    }
   ],
   "source": [
    "# example input2\n",
    "input_range = (0, 2)\n",
    "start, end = input_range\n",
    "\n",
    "print(start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(filename):\n",
    "    local_filename = filename.split('/')[-1]\n",
    "    print('Started downloading: ', local_filename)\n",
    "    print('It may take up to 5 minutes to complete the process.')\n",
    "    with requests.get(filename, stream=True) as r:\n",
    "        with open(local_filename, 'wb') as f:\n",
    "            shutil.copyfileobj(r.raw, f)\n",
    "\n",
    "    return local_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started downloading:  enwiki-20200101-pages-articles-multistream1.xml-p10p30302.bz2\n",
      "Finished downloading:  enwiki-20200101-pages-articles-multistream1.xml-p10p30302.bz2  in Datasets folder\n",
      "Now decompressing the file...\n",
      "Finished decompressing the file.\n",
      "Started downloading:  enwiki-20200101-pages-articles-multistream2.xml-p30304p88444.bz2\n",
      "Finished downloading:  enwiki-20200101-pages-articles-multistream2.xml-p30304p88444.bz2  in Datasets folder\n",
      "Now decompressing the file...\n",
      "Finished decompressing the file.\n"
     ]
    }
   ],
   "source": [
    "for files in available_files[start:end]:\n",
    "    filename = file_page+files\n",
    "    local_file = download_file(filename)\n",
    "    print('Finished downloading: ', local_file, ' in Datasets folder')\n",
    "    print('Now decompressing the file...')\n",
    "\n",
    "    new_file_name = '-'.join(local_file.split('-')[:-1])\n",
    "    comp_file = bz2.BZ2File(local_file, 'rb')\n",
    "    read_file = comp_file.read()\n",
    "    comp_file.close()\n",
    "    f =  open(new_file_name, \"wb\")\n",
    "    f.write(read_file)\n",
    "    f.close()\n",
    "\n",
    "    shutil.move(new_file_name, 'Datasets/'+new_file_name)\n",
    "    os.unlink(local_file)\n",
    "\n",
    "    print('Finished decompressing the file.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test small file\n",
    "#filename = 'https://dumps.wikimedia.org/enwiki/20200101/enwiki-20200101-pages-articles-multistream-index1.txt-p10p30302.bz2'\n",
    "# test large file\n",
    "#filename = 'https://dumps.wikimedia.org/enwiki/20200101/enwiki-20200101-pages-articles-multistream1.xml-p10p30302.bz2'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code for clickstream dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://dumps.wikimedia.org/other/clickstream/'\n",
    "res = requests.get(url)\n",
    "content = str(res.text)\n",
    "html_soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "#print(html_soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2020-01', '2019-12', '2019-11', '2019-10', '2019-09', '2019-08', '2019-07', '2019-06', '2019-05', '2019-04', '2019-03', '2019-02', '2019-01', '2018-12', '2018-11', '2018-10', '2018-09', '2018-08', '2018-07', '2018-06', '2018-05', '2018-04', '2018-03', '2018-02', '2018-01', '2017-12', '2017-11']\n"
     ]
    }
   ],
   "source": [
    "containers = html_soup.find_all('pre')\n",
    "links = containers[0].findAll('a', href=True, text=True)\n",
    "link_list = []\n",
    "for link in links:\n",
    "    link_list.append(link['href'].replace('/', ''))\n",
    "    \n",
    "\n",
    "# currently available text datasets\n",
    "available_month = sorted(set(link_list[1:-1]), reverse=True)\n",
    "print(available_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example input1\n",
    "input_date = '2020-01'\n",
    "\n",
    "file_page = url+input_date+'/'\n",
    "\n",
    "res_page = requests.get(file_page)\n",
    "content_page = str(res_page.text)\n",
    "html_page = BeautifulSoup(content_page, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01\n"
     ]
    }
   ],
   "source": [
    "test_date = '202001'\n",
    "print(test_date[:4]+'-'+test_date[4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://dumps.wikimedia.org/other/clickstream/2020-01/\n"
     ]
    }
   ],
   "source": [
    "print(file_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clickstream-enwiki-2020-01.tsv.gz\n"
     ]
    }
   ],
   "source": [
    "container_link = html_page.find_all('a', href=True, text=True)\n",
    "en_file = ''\n",
    "for link in container_link:\n",
    "    if ('-enwiki-' in link.text):\n",
    "        en_file = link.text\n",
    "\n",
    "print(en_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(filename):\n",
    "    local_filename = filename.split('/')[-1]\n",
    "    print('Started downloading: ', local_filename)\n",
    "    print('It may take up to 10 minutes to complete the process.')\n",
    "    with requests.get(filename, stream=True) as r:\n",
    "        with open(local_filename, 'wb') as f:\n",
    "            shutil.copyfileobj(r.raw, f)\n",
    "\n",
    "    return local_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started downloading:  clickstream-enwiki-2020-01.tsv.gz\n",
      "It may take up to 15 minutes to complete the process.\n",
      "Finished downloading:  clickstream-enwiki-2020-01.tsv.gz  in Datasets folder\n",
      "Now decompressing the file...\n",
      "Finished decompressing the file.\n"
     ]
    }
   ],
   "source": [
    "filename = file_page+en_file\n",
    "local_file = download_file(filename)\n",
    "print('Finished downloading: ', local_file, ' in Datasets folder')\n",
    "print('Now decompressing the file...')\n",
    "\n",
    "comp_file = gzip.open(local_file, 'rb')\n",
    "read_file = comp_file.read()\n",
    "comp_file.close()\n",
    "\n",
    "new_file_name = en_file.replace('.gz', '')\n",
    "f =  open(new_file_name, 'wb')\n",
    "f.write(read_file)\n",
    "f.close()\n",
    "\n",
    "shutil.move(new_file_name, 'Datasets/'+new_file_name)\n",
    "os.unlink(local_file)\n",
    "\n",
    "print('Finished decompressing the file.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
